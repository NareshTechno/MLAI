{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all necessary libraries\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "pd.set_option('max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's build a basic bag of words model on three sample documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Gangs of Wasseypur is a great movie.', 'The success of a movie depends on the performance of the actors.', 'There are no new movies releasing this week.']\n"
     ]
    }
   ],
   "source": [
    "documents = [\"Gangs of Wasseypur is a great movie.\", \"The success of a movie depends on the performance of the actors.\", \"There are no new movies releasing this week.\"]\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gangs wasseypur great movie .', 'success movie depends performance actors .', 'new movies releasing week .']\n"
     ]
    }
   ],
   "source": [
    "def preprocess(document):\n",
    "    'changes document to lower case and removes stopwords'\n",
    "\n",
    "    # change sentence to lower case\n",
    "    document = document.lower()\n",
    "\n",
    "    # tokenize into words\n",
    "    words = word_tokenize(document)\n",
    "\n",
    "    # remove stop words\n",
    "    words = [word for word in words if word not in stopwords.words(\"english\")]\n",
    "\n",
    "    # join words to make sentence\n",
    "    document = \" \".join(words)\n",
    "    \n",
    "    return document\n",
    "\n",
    "documents = [preprocess(document) for document in documents]\n",
    "print(documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating bag of words model using count vectorizer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 2)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 7)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 9)\t1\n",
      "  (1, 4)\t1\n",
      "  (2, 11)\t1\n",
      "  (2, 8)\t1\n",
      "  (2, 5)\t1\n",
      "  (2, 6)\t1\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "bow_model = vectorizer.fit_transform(documents)\n",
    "print(bow_model)  # returns the row number and column number of the cells which have 1 as value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 1 1 0 0 0 0 0 1 0]\n",
      " [1 1 0 0 1 0 0 1 0 1 0 0]\n",
      " [0 0 0 0 0 1 1 0 1 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# print the full sparse matrix\n",
    "print(bow_model.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 12)\n",
      "['actors', 'depends', 'gangs', 'great', 'movie', 'movies', 'new', 'performance', 'releasing', 'success', 'wasseypur', 'week']\n"
     ]
    }
   ],
   "source": [
    "print(bow_model.shape)\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's create a bag of words model on the spam dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = Path(\"C:/Upgrad Projects/NLP-M1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1   ham   \n",
       "2  spam   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                               message  \n",
       "0  Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...  \n",
       "1                                                                        Ok lar... Joking wif u oni...  \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...  \n",
       "3                                                    U dun say so early hor... U c already then say...  \n",
       "4                                        Nah I don't think he goes to usf, he lives around here though  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "spam = pd.read_csv(folder/\"SMSSpamCollection.txt\", sep = \"\\t\", names=[\"label\", \"message\"])\n",
    "spam.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's take a subset of data (first 50 rows only) and create bag of word model on that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label  \\\n",
      "0    ham   \n",
      "1    ham   \n",
      "2   spam   \n",
      "3    ham   \n",
      "4    ham   \n",
      "5   spam   \n",
      "6    ham   \n",
      "7    ham   \n",
      "8   spam   \n",
      "9   spam   \n",
      "10   ham   \n",
      "11  spam   \n",
      "12  spam   \n",
      "13   ham   \n",
      "14   ham   \n",
      "15  spam   \n",
      "16   ham   \n",
      "17   ham   \n",
      "18   ham   \n",
      "19  spam   \n",
      "20   ham   \n",
      "21   ham   \n",
      "22   ham   \n",
      "23   ham   \n",
      "24   ham   \n",
      "25   ham   \n",
      "26   ham   \n",
      "27   ham   \n",
      "28   ham   \n",
      "29   ham   \n",
      "30   ham   \n",
      "31   ham   \n",
      "32   ham   \n",
      "33   ham   \n",
      "34  spam   \n",
      "35   ham   \n",
      "36   ham   \n",
      "37   ham   \n",
      "38   ham   \n",
      "39   ham   \n",
      "40   ham   \n",
      "41   ham   \n",
      "42  spam   \n",
      "43   ham   \n",
      "44   ham   \n",
      "45   ham   \n",
      "46   ham   \n",
      "47   ham   \n",
      "48   ham   \n",
      "49   ham   \n",
      "\n",
      "                                                                                                message  \n",
      "0   Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...  \n",
      "1                                                                         Ok lar... Joking wif u oni...  \n",
      "2   Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...  \n",
      "3                                                     U dun say so early hor... U c already then say...  \n",
      "4                                         Nah I don't think he goes to usf, he lives around here though  \n",
      "5   FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for ...  \n",
      "6                         Even my brother is not like to speak with me. They treat me like aids patent.  \n",
      "7   As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...  \n",
      "8   WINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To c...  \n",
      "9   Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with came...  \n",
      "10  I'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? I've cried ...  \n",
      "11  SIX chances to win CASH! From 100 to 20,000 pounds txt> CSH11 and send to 87575. Cost 150p/day, ...  \n",
      "12  URGENT! You have won a 1 week FREE membership in our £100,000 Prize Jackpot! Txt the word: CLAIM...  \n",
      "13  I've been searching for the right words to thank you for this breather. I promise i wont take yo...  \n",
      "14                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!  \n",
      "15  XXXMobileMovieClub: To use your credit, click the WAP link in the next txt message or click here...  \n",
      "16                                                                           Oh k...i'm watching here:)  \n",
      "17                    Eh u remember how 2 spell his name... Yes i did. He v naughty make until i v wet.  \n",
      "18                                             Fine if thats the way u feel. Thats the way its gota b  \n",
      "19  England v Macedonia - dont miss the goals/team news. Txt ur national team to 87077 eg ENGLAND to...  \n",
      "20                                                            Is that seriously how you spell his name?  \n",
      "21                                                      I‘m going to try for 2 months ha ha only joking  \n",
      "22                                                 So ü pay first lar... Then when is da stock comin...  \n",
      "23             Aft i finish my lunch then i go str down lor. Ard 3 smth lor. U finish ur lunch already?  \n",
      "24                                            Ffffffffff. Alright no way I can meet up with you sooner?  \n",
      "25  Just forced myself to eat a slice. I'm really not hungry tho. This sucks. Mark is getting worrie...  \n",
      "26                                                                       Lol your always so convincing.  \n",
      "27  Did you catch the bus ? Are you frying an egg ? Did you make a tea? Are you eating your mom's le...  \n",
      "28                          I'm back &amp; we're packing the car now, I'll let you know if there's room  \n",
      "29                                     Ahhh. Work. I vaguely remember that! What does it feel like? Lol  \n",
      "30  Wait that's still not all that clear, were you not sure about me being sarcastic or that that's ...  \n",
      "31  Yeah he got in at 2 and was v apologetic. n had fallen out and she was actin like spoilt child a...  \n",
      "32                                                                        K tell me anything about you.  \n",
      "33                 For fear of fainting with the of all that housework you just did? Quick have a cuppa  \n",
      "34  Thanks for your subscription to Ringtone UK your mobile will be charged £5/month Please confirm ...  \n",
      "35  Yup... Ok i go home look at the timings then i msg ü again... Xuhui going to learn on 2nd may to...  \n",
      "36                                                      Oops, I'll let you know when my roommate's done  \n",
      "37                                                                         I see the letter B on my car  \n",
      "38                                                                          Anything lor... U decide...  \n",
      "39  Hello! How's you and how did saturday go? I was just texting to see if you'd decided to do anyth...  \n",
      "40                   Pls go ahead with watts. I just wanted to be sure. Do have a great weekend. Abiola  \n",
      "41  Did I forget to tell you ? I want you , I need you, I crave you ... But most of all ... I love y...  \n",
      "42  07732584351 - Rodger Burns - MSG = We tried to call you re your reply to our sms for a free noki...  \n",
      "43                                                                                  WHO ARE YOU SEEING?  \n",
      "44                             Great! I hope you like your man well endowed. I am  &lt;#&gt;  inches...  \n",
      "45                                                                     No calls..messages..missed calls  \n",
      "46                                                        Didn't you get hep b immunisation in nigeria.  \n",
      "47                                                                      Fair enough, anything going on?  \n",
      "48                                  Yeah hopefully, if tyler can't do it I could maybe ask around a bit  \n",
      "49  U don't know how stubborn I am. I didn't even want to go to the hospital. I kept telling Mark I'...  \n"
     ]
    }
   ],
   "source": [
    "spam = spam.iloc[0:50,:]\n",
    "print(spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the messages from the dataframe\n",
    "messages = spam.message\n",
    "#print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert messages into list\n",
    "messages = [message for message in messages]\n",
    "#print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess messages using the preprocess function\n",
    "messages = [preprocess(message) for message in messages]\n",
    "#print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# bag of words model\n",
    "vectorizer = CountVectorizer()\n",
    "bow_model = vectorizer.fit_transform(messages)\n",
    "print(bow_model.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 381)\n",
      "['000', '07732584351', '08000930705', '08002986030', '08452810075over18', '09061701461', '100', '11', '12', '150p', '16', '20', '2005', '21st', '2nd', '4403ldnw1a7rw18', '4txt', '50', '6days', '81010', '87077', '87121', '87575', '8am', '900', 'abiola', 'actin', 'aft', 'ahead', 'ahhh', 'aids', 'already', 'alright', 'always', 'amore', 'amp', 'anymore', 'anything', 'apologetic', 'apply', 'arabian', 'ard', 'around', 'ask', 'available', 'back', 'badly', 'bit', 'blessing', 'breather', 'brother', 'buffet', 'bugis', 'burns', 'bus', 'ca', 'call', 'callers', 'callertune', 'calls', 'camcorder', 'camera', 'car', 'cash', 'catch', 'caught', 'chances', 'charged', 'cheers', 'chgs', 'child', 'cine', 'claim', 'clear', 'click', 'co', 'code', 'colour', 'com', 'comin', 'comp', 'confirm', 'convincing', 'copy', 'cost', 'could', 'crave', 'crazy', 'credit', 'cried', 'csh11', 'cup', 'cuppa', 'customer', 'da', 'darling', 'date', 'day', 'dbuk', 'decide', 'decided', 'delivery', 'dinner', 'done', 'dont', 'dun', 'early', 'eat', 'eating', 'eg', 'egg', 'eh', 'endowed', 'england', 'enough', 'entitled', 'entry', 'even', 'fa', 'fainting', 'fair', 'fallen', 'fear', 'feel', 'ffffffffff', 'final', 'fine', 'finish', 'first', 'forced', 'forget', 'free', 'freemsg', 'friends', 'frying', 'fulfil', 'fun', 'get', 'getting', 'go', 'goals', 'goes', 'going', 'gon', 'got', 'gota', 'granted', 'great', 'gt', 'ha', 'hello', 'help', 'hep', 'hey', 'hl', 'home', 'hope', 'hopefully', 'hor', 'hospital', 'hospitals', 'hours', 'housework', 'http', 'hungry', 'immunisation', 'inches', 'info', 'invite', 'jackpot', 'joking', 'jurong', 'kept', 'kl341', 'know', 'knows', 'la', 'lar', 'latest', 'lccltd', 'learn', 'left', 'lesson', 'let', 'letter', 'like', 'link', 'live', 'lives', 'll', 'lol', 'look', 'lor', 'love', 'lt', 'lunch', 'macedonia', 'make', 'man', 'mark', 'may', 'maybe', 'meet', 'melle', 'membership', 'message', 'messages', 'minnaminunginte', 'miss', 'missed', 'mmmmmm', 'mobile', 'mobiles', 'mom', 'month', 'months', 'msg', 'na', 'nah', 'name', 'national', 'naughty', 'need', 'net', 'network', 'news', 'next', 'nigeria', 'nokia', 'nurungu', 'oh', 'ok', 'oni', 'oops', 'oru', 'packing', 'patent', 'pay', 'per', 'pizza', 'please', 'pls', 'pobox', 'poboxox36504w45wq', 'point', 'pounds', 'press', 'prize', 'promise', 'qjkgighjjgcbl', 'question', 'quick', 'rate', 'rcv', 're', 'really', 'receive', 'receivea', 'remember', 'reply', 'replying', 'request', 'reward', 'right', 'ringtone', 'rodger', 'room', 'roommate', 'sarcastic', 'saturday', 'say', 'scotland', 'searching', 'see', 'seeing', 'selected', 'send', 'seriously', 'set', 'sick', 'six', 'slice', 'sms', 'smth', 'soon', 'sooner', 'speak', 'spell', 'spoilt', 'std', 'steed', 'still', 'stock', 'str', 'stubborn', 'stuff', 'subscription', 'sucker', 'suckers', 'sucks', 'sunday', 'sure', 'sweet', 'take', 'talk', 'tb', 'tea', 'team', 'tell', 'telling', 'text', 'texting', 'thank', 'thanks', 'that', 'think', 'tho', 'though', 'till', 'times', 'timings', 'tkts', 'today', 'tomo', 'tomorrow', 'tonight', 'treat', 'tried', 'try', 'trying', 'tsandcs', 'turn', 'txt', 'tyler', 'uk', 'update', 'ur', 'urgent', 'us', 'use', 'usf', 'vaguely', 'valid', 'valued', 've', 'vettam', 'wait', 'wales', 'want', 'wanted', 'wap', 'wat', 'watching', 'watts', 'way', 'weak', 'week', 'weekend', 'well', 'wet', 'wif', 'win', 'winner', 'wkly', 'wo', 'wonderful', 'wont', 'word', 'words', 'work', 'world', 'worried', 'www', 'xuhui', 'xxx', 'xxxmobilemovieclub', 'yeah', 'yes', 'yummy', 'yup', 'ú1']\n"
     ]
    }
   ],
   "source": [
    "print(bow_model.shape)\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A lot of duplicate tokens such as 'win'and 'winner'; 'reply' and 'replying'; 'want' and 'wanted' etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and lemmatising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# add stemming and lemmatisation in the preprocess function\n",
    "def preprocess(document, stem=True):\n",
    "    'changes document to lower case and removes stopwords'\n",
    "\n",
    "    # change sentence to lower case\n",
    "    document = document.lower()\n",
    "\n",
    "    # tokenize into words\n",
    "    words = word_tokenize(document)\n",
    "\n",
    "    # remove stop words\n",
    "    words = [word for word in words if word not in stopwords.words(\"english\")]\n",
    "    \n",
    "    if stem:\n",
    "        words = [stemmer.stem(word) for word in words]\n",
    "    else:\n",
    "        words = [wordnet_lemmatizer.lemmatize(word, pos='v') for word in words]\n",
    "\n",
    "    # join words to make sentence\n",
    "    document = \" \".join(words)\n",
    "    \n",
    "    return document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words model on stemmed messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# stem messages\n",
    "messages = [preprocess(message, stem=True) for message in spam.message]\n",
    "\n",
    "# bag of words model\n",
    "vectorizer = CountVectorizer()\n",
    "bow_model = vectorizer.fit_transform(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# look at the dataframe\n",
    "bowstemdf = pd.DataFrame(bow_model.toarray(), columns = vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 359)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bowstemdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['000', '07732584351', '08000930705', '08002986030', '08452810075over18', '09061701461', '100', '11', '12', '150p', '16', '20', '2005', '21st', '2nd', '4403ldnw1a7rw18', '4txt', '50', '6day', '81010', '87077', '87121', '87575', '8am', '900', 'abiola', 'actin', 'aft', 'ahead', 'ahhh', 'aid', 'alreadi', 'alright', 'alway', 'amor', 'amp', 'anymor', 'anyth', 'apologet', 'appli', 'arabian', 'ard', 'around', 'ask', 'avail', 'back', 'badli', 'bit', 'bless', 'breather', 'brother', 'bu', 'buffet', 'bugi', 'burn', 'ca', 'call', 'caller', 'callertun', 'calls', 'camcord', 'camera', 'car', 'cash', 'catch', 'caught', 'chanc', 'charg', 'cheer', 'chg', 'child', 'cine', 'claim', 'clear', 'click', 'co', 'code', 'colour', 'com', 'comin', 'comp', 'confirm', 'convinc', 'copi', 'cost', 'could', 'crave', 'crazy', 'credit', 'cri', 'csh11', 'cup', 'cuppa', 'custom', 'da', 'darl', 'date', 'day', 'dbuk', 'decid', 'deliveri', 'dinner', 'done', 'dont', 'dun', 'earli', 'eat', 'eg', 'egg', 'eh', 'endow', 'england', 'enough', 'entitl', 'entri', 'even', 'fa', 'faint', 'fair', 'fallen', 'fear', 'feel', 'ffffffffff', 'final', 'fine', 'finish', 'first', 'forc', 'forget', 'free', 'freemsg', 'fri', 'friend', 'fulfil', 'fun', 'get', 'go', 'goals', 'goe', 'gon', 'got', 'gota', 'grant', 'great', 'gt', 'ha', 'hello', 'help', 'hep', 'hey', 'hl', 'home', 'hope', 'hor', 'hospit', 'hour', 'housework', 'http', 'hungri', 'immunis', 'inch', 'info', 'invit', 'jackpot', 'joke', 'jurong', 'kept', 'kl341', 'know', 'la', 'lar', 'latest', 'lccltd', 'learn', 'left', 'lesson', 'let', 'letter', 'like', 'link', 'live', 'll', 'lol', 'look', 'lor', 'love', 'lt', 'lunch', 'macedonia', 'make', 'man', 'mark', 'may', 'mayb', 'meet', 'mell', 'membership', 'messag', 'messages', 'minnaminungint', 'miss', 'mmmmmm', 'mobil', 'mom', 'month', 'msg', 'na', 'nah', 'name', 'nation', 'naughti', 'need', 'net', 'network', 'news', 'next', 'nigeria', 'nokia', 'nurungu', 'oh', 'ok', 'oni', 'oop', 'oru', 'pack', 'patent', 'pay', 'per', 'pizza', 'pl', 'pleas', 'pobox', 'poboxox36504w45wq', 'point', 'pound', 'press', 'prize', 'promis', 'qjkgighjjgcbl', 'question', 'quick', 'rate', 'rcv', 're', 'realli', 'receiv', 'receivea', 'rememb', 'repli', 'request', 'reward', 'right', 'rington', 'rodger', 'room', 'roommat', 'sarcast', 'saturday', 'say', 'scotland', 'search', 'see', 'select', 'send', 'serious', 'set', 'sick', 'six', 'slice', 'sm', 'smth', 'soon', 'sooner', 'speak', 'spell', 'spoilt', 'std', 'steed', 'still', 'stock', 'str', 'stubborn', 'stuff', 'subscript', 'suck', 'sucker', 'sunday', 'sure', 'sweet', 'take', 'talk', 'tb', 'tea', 'team', 'tell', 'text', 'thank', 'that', 'think', 'tho', 'though', 'till', 'time', 'tkt', 'today', 'tomo', 'tomorrow', 'tonight', 'treat', 'tri', 'tsandc', 'turn', 'txt', 'tyler', 'uk', 'updat', 'ur', 'urgent', 'us', 'use', 'usf', 'vagu', 'valid', 'valu', 've', 'vettam', 'wait', 'wale', 'want', 'wap', 'wat', 'watch', 'watt', 'way', 'weak', 'week', 'weekend', 'well', 'wet', 'wif', 'win', 'winner', 'wkli', 'wo', 'wonder', 'wont', 'word', 'work', 'world', 'worri', 'www', 'xuhui', 'xxx', 'xxxmobilemovieclub', 'ye', 'yeah', 'yummi', 'yup', 'ú1']\n"
     ]
    }
   ],
   "source": [
    "# token names\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 359 tokens after stemming the messages as compared to 381 tokens without stemming.\n",
    "\n",
    "### Let's try lemmatizing the messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lemmatise messages\n",
    "messages = [preprocess(message, stem=False) for message in spam.message]\n",
    "\n",
    "# bag of words model\n",
    "vectorizer = CountVectorizer()\n",
    "bow_model = vectorizer.fit_transform(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# look at the dataframe\n",
    "bowlemdf = pd.DataFrame(bow_model.toarray(), columns = vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 363)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bowlemdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['000', '07732584351', '08000930705', '08002986030', '08452810075over18', '09061701461', '100', '11', '12', '150p', '16', '20', '2005', '21st', '2nd', '4403ldnw1a7rw18', '4txt', '50', '6days', '81010', '87077', '87121', '87575', '8am', '900', 'abiola', 'actin', 'aft', 'ahead', 'ahhh', 'aid', 'already', 'alright', 'always', 'amore', 'amp', 'anymore', 'anything', 'apologetic', 'apply', 'arabian', 'ard', 'around', 'ask', 'available', 'back', 'badly', 'bite', 'bless', 'breather', 'brother', 'buffet', 'bugis', 'burn', 'bus', 'ca', 'call', 'callers', 'callertune', 'calls', 'camcorder', 'camera', 'car', 'cash', 'catch', 'chance', 'charge', 'cheer', 'chgs', 'child', 'cine', 'claim', 'clear', 'click', 'co', 'code', 'colour', 'com', 'comin', 'comp', 'confirm', 'convince', 'copy', 'cost', 'could', 'crave', 'crazy', 'credit', 'cry', 'csh11', 'cup', 'cuppa', 'customer', 'da', 'darling', 'date', 'day', 'dbuk', 'decide', 'delivery', 'dinner', 'do', 'dont', 'dun', 'early', 'eat', 'eg', 'egg', 'eh', 'endow', 'england', 'enough', 'entitle', 'entry', 'even', 'fa', 'faint', 'fair', 'fall', 'fear', 'feel', 'ffffffffff', 'final', 'fine', 'finish', 'first', 'force', 'forget', 'free', 'freemsg', 'friends', 'fry', 'fulfil', 'fun', 'get', 'go', 'goals', 'gon', 'gota', 'grant', 'great', 'gt', 'ha', 'hello', 'help', 'hep', 'hey', 'hl', 'home', 'hope', 'hopefully', 'hor', 'hospital', 'hospitals', 'hours', 'housework', 'http', 'hungry', 'immunisation', 'inch', 'info', 'invite', 'jackpot', 'joke', 'jurong', 'keep', 'kl341', 'know', 'la', 'lar', 'latest', 'lccltd', 'learn', 'leave', 'lesson', 'let', 'letter', 'like', 'link', 'live', 'll', 'lol', 'look', 'lor', 'love', 'lt', 'lunch', 'macedonia', 'make', 'man', 'mark', 'may', 'maybe', 'meet', 'melle', 'membership', 'message', 'messages', 'minnaminunginte', 'miss', 'missed', 'mmmmmm', 'mobile', 'mobiles', 'mom', 'month', 'months', 'msg', 'na', 'nah', 'name', 'national', 'naughty', 'need', 'net', 'network', 'news', 'next', 'nigeria', 'nokia', 'nurungu', 'oh', 'ok', 'oni', 'oops', 'oru', 'pack', 'patent', 'pay', 'per', 'pizza', 'please', 'pls', 'pobox', 'poboxox36504w45wq', 'point', 'pound', 'press', 'prize', 'promise', 'qjkgighjjgcbl', 'question', 'quick', 'rate', 'rcv', 're', 'really', 'receive', 'receivea', 'remember', 'reply', 'request', 'reward', 'right', 'ringtone', 'rodger', 'room', 'roommate', 'sarcastic', 'saturday', 'say', 'scotland', 'search', 'see', 'select', 'send', 'seriously', 'set', 'sick', 'six', 'slice', 'sms', 'smth', 'soon', 'sooner', 'speak', 'spell', 'spoil', 'std', 'steed', 'still', 'stock', 'str', 'stubborn', 'stuff', 'subscription', 'suck', 'sucker', 'suckers', 'sunday', 'sure', 'sweet', 'take', 'talk', 'tb', 'tea', 'team', 'tell', 'text', 'texting', 'thank', 'that', 'think', 'tho', 'though', 'till', 'time', 'tkts', 'today', 'tomo', 'tomorrow', 'tonight', 'treat', 'try', 'tsandcs', 'turn', 'txt', 'tyler', 'uk', 'update', 'ur', 'urgent', 'us', 'use', 'usf', 'vaguely', 'valid', 'value', 've', 'vettam', 'wait', 'wales', 'want', 'wap', 'wat', 'watch', 'watts', 'way', 'weak', 'week', 'weekend', 'well', 'wet', 'wif', 'win', 'winner', 'wkly', 'wo', 'wonderful', 'wont', 'word', 'work', 'world', 'worry', 'www', 'xuhui', 'xxx', 'xxxmobilemovieclub', 'yeah', 'yes', 'yummy', 'yup', 'ú1']\n"
     ]
    }
   ],
   "source": [
    "# token names\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 363 tokens after lemmatizing the messages as compared to 381 tokens without lemmatising. But, on the other hand, stemmer reduces the token count to 359. Lemmatization doesn't work as expected because the data is very unclean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363\n"
     ]
    }
   ],
   "source": [
    "print(len(vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "Document1 = \"Vapour, Bangalore has a really great terrace seating and an awesome view of the Bangalore skyline\"\n",
    "Document2 = \"The beer at Vapour, Bangalore was amazing. My favourites are the wheat beer and the ale beer.\"\n",
    "Document3 = \"Vapour, Bangalore has the best view in Bangalore.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [Document1, Document2, Document3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(document):\n",
    "    'changes document to lower case and removes stopwords'\n",
    "\n",
    "    # change sentence to lower case\n",
    "    document = document.lower()\n",
    "\n",
    "    # tokenize into words\n",
    "    words = word_tokenize(document)\n",
    "\n",
    "    # remove stop words\n",
    "    words = [word for word in words if word not in stopwords.words(\"english\")]\n",
    "\n",
    "    # join words to make sentence\n",
    "    document = \" \".join(words)\n",
    "    \n",
    "    return document\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vapour , bangalore really great terrace seating awesome view bangalore skyline', 'beer vapour , bangalore amazing . favourites wheat beer ale beer .', 'vapour , bangalore best view bangalore .']\n"
     ]
    }
   ],
   "source": [
    "documents = [preprocess(document) for document in documents]\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_model = vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ale</th>\n",
       "      <th>amazing</th>\n",
       "      <th>awesome</th>\n",
       "      <th>bangalore</th>\n",
       "      <th>beer</th>\n",
       "      <th>best</th>\n",
       "      <th>favourites</th>\n",
       "      <th>great</th>\n",
       "      <th>really</th>\n",
       "      <th>seating</th>\n",
       "      <th>skyline</th>\n",
       "      <th>terrace</th>\n",
       "      <th>vapour</th>\n",
       "      <th>view</th>\n",
       "      <th>wheat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ale  amazing  awesome  bangalore  beer  best  favourites  great  really  \\\n",
       "0    0        0        1          2     0     0           0      1       1   \n",
       "1    1        1        0          1     3     0           1      0       0   \n",
       "2    0        0        0          2     0     1           0      0       0   \n",
       "\n",
       "   seating  skyline  terrace  vapour  view  wheat  \n",
       "0        1        1        1       1     1      0  \n",
       "1        0        0        0       1     0      1  \n",
       "2        0        0        0       1     1      0  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at the dataframe\n",
    "bowdf = pd.DataFrame(bow_model.toarray(), columns = vectorizer.get_feature_names())\n",
    "bowdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tot_terms = bowdf.iloc[0].astype(bool).sum()\n",
    "tot_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfreq = bowdf.loc[0,'bangalore']\n",
    "tfreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf = round(tfreq/tot_terms,2)\n",
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tot_docs = len(bowdf.index)\n",
    "tot_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_count(term):\n",
    "    sum = 0\n",
    "    if term > 0:\n",
    "        sum = sum + 1\n",
    "    \n",
    "    return sum\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dterm_cnt = bowdf.apply(lambda row: term_count(row['bangalore']), axis=1).sum()\n",
    "dterm_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dterm_cnt = bowdf['bangalore'].astype(bool).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf = round(math.log10(tot_docs/dterm_cnt),2)\n",
    "idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf = round(tf * idf, 2)\n",
    "tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctf(df, document_no,term):\n",
    "    print(\"Calculating tf\")\n",
    "    tfreq     = df.loc[document_no,term]\n",
    "    print(tfreq)\n",
    "    tot_terms = df.iloc[document_no].sum()\n",
    "    print(tot_terms)\n",
    "    tf = round(tfreq/tot_terms,3)\n",
    "    return tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cidf(df, term):\n",
    "    print(\"Calculating idf\")\n",
    "    tot_docs = len(df.index)\n",
    "    print(tot_docs)\n",
    "    term_in_docs_cnt = df[term].astype(bool).sum()\n",
    "    print(term_in_docs_cnt)\n",
    "    idf = round(math.log10(tot_docs/term_in_docs_cnt),3)\n",
    "    return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating tf\n",
      "2\n",
      "10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.20000000000000001"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf = ctf(bowdf, 0, 'bangalore')\n",
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating idf\n",
      "3\n",
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf = cidf(bowdf, 'bangalore')\n",
    "idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf = round(tf * idf, 2)\n",
    "tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating tf\n",
      "3\n",
      "9\n",
      "0.333\n",
      "Calculating idf\n",
      "3\n",
      "1\n",
      "0.477\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.159"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf = ctf(bowdf, 1, 'beer')\n",
    "print(tf)\n",
    "idf = cidf(bowdf, 'beer')\n",
    "print(idf)\n",
    "tf_idf = float(format(tf * idf, '.3f'))\n",
    "tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bowdf.iloc[1].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bowdf.loc[1,'beer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33333333333333331"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf = bowdf.loc[1,'beer']/bowdf.iloc[1].sum()\n",
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bowdf.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bowdf['beer'].astype(bool).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47712125471966244"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf = math.log10((len(bowdf.index)/bowdf['beer'].astype(bool).sum()))\n",
    "idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.159'"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf = format(tf * idf, '.3f')\n",
    "tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating tf\n",
      "1\n",
      "9\n",
      "0.111\n",
      "Calculating idf\n",
      "3\n",
      "3\n",
      "0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf = ctf(bowdf, 1, 'bangalore')\n",
    "print(tf)\n",
    "idf = cidf(bowdf, 'bangalore')\n",
    "print(idf)\n",
    "tf_idf = float(format(tf * idf, '.3f'))\n",
    "tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating tf\n",
      "1\n",
      "9\n",
      "0.111\n",
      "Calculating idf\n",
      "3\n",
      "3\n",
      "0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf = ctf(bowdf, 1, 'vapour')\n",
    "print(tf)\n",
    "idf = cidf(bowdf, 'vapour')\n",
    "print(idf)\n",
    "tf_idf = float(format(tf * idf, '.3f'))\n",
    "tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating tf\n",
      "2\n",
      "10\n",
      "0.2\n",
      "Calculating idf\n",
      "3\n",
      "3\n",
      "0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf = ctf(bowdf, 0, 'bangalore')\n",
    "print(tf)\n",
    "idf = cidf(bowdf, 'bangalore')\n",
    "print(idf)\n",
    "tf_idf = float(format(tf * idf, '.3f'))\n",
    "tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating tf\n",
      "1\n",
      "9\n",
      "0.111\n",
      "Calculating idf\n",
      "3\n",
      "3\n",
      "0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf = ctf(bowdf, 1, 'bangalore')\n",
    "print(tf)\n",
    "idf = cidf(bowdf, 'bangalore')\n",
    "print(idf)\n",
    "tf_idf = float(format(tf * idf, '.3f'))\n",
    "tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating tf\n",
      "2\n",
      "5\n",
      "0.4\n",
      "Calculating idf\n",
      "3\n",
      "3\n",
      "0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf = ctf(bowdf, 2, 'bangalore')\n",
    "print(tf)\n",
    "idf = cidf(bowdf, 'bangalore')\n",
    "print(idf)\n",
    "tf_idf = float(format(tf * idf, '.3f'))\n",
    "tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
